% Template for ICIP-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\graphicspath{ {./images/} }

% Title.
% ------
\title{LOCATING PROJECTILE IMPACT LOCATIONS ON SHOOTING TARGETS}
%
% Single address.
% ---------------
\name{Brandon Nguyen}
\address{The University of Texas\\
         EE 371R, Electrical and Computer Engineering}

\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
It is shown that by utilizing techniques such as scaling, filtering, and the
circle Hough transform that the points of impact on a target can be programatically located so that
more precise and sophisticated methods can be used to quickly calculate accuracy and precision metrics.
\end{abstract}
%
\begin{keywords}
Application, Circle Detection
\end{keywords}
%
\section{Background}
\label{sec:background}
A shooter requires feedback in the form of accuracy and precision metrics in order
to make adjustments to their zero (affected by accuracy) and see what their level
of performance is (precision).

Accuracy is determined by the "mean point of impact", which is the average location
of the population of points of impact. Shooters will typically visually estimate
the main point of impact since it is often impractical to find the coordinates
of each point of impact to perform the calculation.

Precision is a metric which has several different methods to determine.
The typical method used, due to its simplicity, is "group size" where the distance
between the two furthest points of impact is determined. Shooters will typically visually
pick the two points and measure the distance using, if they have one, a ruler or
informal tools such as their fingers.

The two methods that shooters use above are inadequate for precisely determining
accuracy and precision. Visual estimation of the mean point of impact and two 
furthest points is imprecise and may lead to an inaccurate results. In addition,
the "group size" method used by shooters does not take into account outliers ("fliers")
which may result from an exceptional event such as a gust of wind or muscle spasm.

As noted above, it is impractical to perform other, more computationally precise methods.
Such methods require more time standing at a target which may inconvenience other shooters
if the targets are manually placed or cut into the shooter's own time at the range.

\section{Tools and Materials}
\label{sec:tools}
OpenCV is a critical component of this project. OpenCV comes packaged with functions for
applying a multitude of filters, as well as the circle Hough transform which internally uses a
Canny edge dectector before running the accumulator.

The Python bindings for OpenCV were used in order to quickly prototype the methods used.
An Android application is also being developed for mobile devices.
Due to potential marketability, the source of the Android application will not be available.
However, the Python prototype is publically accessible.

The image data was collected personally. The targets used were of the "reactive" type, which
change color around where projectiles impact, NRA bullseye targets with a coarse stock, and
zeroing targets with grids to aid in zeroing sights. The calibers tested were .22 LR,
.223 Remington / 5.56x45 mm NATO, and 9x19 mm Parabellum.

\section{Methods}
\label{sec:methods}

\subsection{Overview}
\label{ssec:overview}
Inputs to the system are the image of a target (or region of interest of an image to reduce
compute usage), a pixel to physical distance conversion for the image,
and the projectile diameter (caliber). Though ideally only a picture would
be required to locate the impact locations, knowledge of the conversion factor and projectile
diameter nearly eliminates image scaling issues.

With the knowledge of projectile diameter and a conversion factor for pixel to physical distance,
the image can be scaled so that impacts regardless of projectile diameter will be of the roughly
the same pixel radius. This reduces the complexity of the preprocessing steps and circle Hough
transform as now all circles will around the same size in pixels.
Let \(C\ \unit{\frac{pixels}{distance\ units}}\) be the conversion factor, \(N\ \unit{pixels}\)
be the normalized projectile radius in the image space, \(R\ \unit{distance\ units}\) be the 
actual radius of the projectile, \(S\) be the scaling factor for the image, \(D\) be 
the dimensions of the original image, and \(\bar{D}\) be the dimensions of the scaled image.

Thus:

\[S = \frac{N}{R * C}\]

\[\bar{D} = D * S\]

The image is initially converted to grayscale as intensity data and not color data is required
to pick out the structures of the impacts.
Preprocessing steps involving filtering are performed on the image. These steps are done
in context of the circle Hough transform being used to locate the impacts.
First, an even blur kernel is applied to the image in order to soften edges. This is so that
ragged edges of the impacts are smoothed out. Next, a Gaussian kernel is applied in order
to smooth out naturally occuring noise. Next, a median filter and bilateral filter are used in order
to create larger patches of the same intensity to prevent small edges from causing false circles.

The circle Hough transform is then performed, resulting in the circles and their radii being returned.

\subsection{Implementation}
\label{ssec:implementation}

A Python prototype was created with an Android application in the works utilizing OpenCV.
The Python prototype takes as an argument the path to an image file, prompts
the user for the projectile diameter in inches (the United States shooting industry
still revolves around Imperial units), and then prompts the user to draw a line
on the original image as the pixel length to map to the real distance the user then inputs.
At this point the necessary scaling factor is calculated.
The user is then prompted to select a region of interest on the original target image.
That subimage is then scaled, preprocessed, and has the circle Hough transform run on it.
The circles returned have their coordinates and radii in the scaled image space, so a
transformation step is performed to provide coordinates and radii in the dimensions of
the original image. These circles are then drawn onto an output image, with the mean
point of impact being calculated and depicted as a cross.

The Android application allows users to take photos or use one from their gallery. Similarly,
it prompts the user for projectile diameter, pixel-to-real-distance conversion factor, and
a region of interest on the image. It is in the Android application where careful management
of buffers is performed to keep the process performant on mobile devices:
there is very little delay when performing the circle Hough transform even on
images with high resolutions (e.g. 4608x3456).

\section{Results}
\label{sec:results}

\newcommand{\showsteps}[1] {
    \begin{figure}[htb]
    \begin{minipage}[b]{0.5\linewidth}
      \centering
      \centerline{\includegraphics[width=4.0cm]{out_1_#1}}
      \centerline{(1) Selected region}\medskip
    \end{minipage}%
    \begin{minipage}[b]{0.5\linewidth}
      \centering
      \centerline{\includegraphics[width=4.0cm]{out_2_#1}}
      \centerline{(2) Preprocessing performed}\medskip
    \end{minipage}
    
    \begin{minipage}[b]{0.5\linewidth}
      \centering
      \centerline{\includegraphics[width=4.0cm]{out_3_#1}}
      \centerline{(3) Canny edge map}\medskip
    \end{minipage}%
    \begin{minipage}[b]{0.5\linewidth}
      \centering
      \centerline{\includegraphics[width=4.0cm]{out_4_#1}}
      \centerline{(4) Resulting circles}\medskip
    \end{minipage}
    \caption{State of target#1.jpg through the process}
    \label{fig:res}
    \end{figure}
}

\showsteps{0}
\showsteps{1}
\showsteps{3}
\showsteps{8}
\showsteps{23}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
